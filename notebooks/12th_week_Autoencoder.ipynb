{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93b2eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import IPython.display as ipd\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2885007",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Let's only use testset\n",
    "'''\n",
    "!wget http://download.magenta.tensorflow.org/datasets/nsynth/nsynth-test.jsonwav.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f774dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xf nsynth-test.jsonwav.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae6092c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NSynthDataSet:\n",
    "  def __init__(self, path):\n",
    "    if isinstance(path, str):\n",
    "      path = Path(path)\n",
    "    self.path = path\n",
    "    json_path = path / \"examples.json\"\n",
    "    self.meta = pd.read_json(json_path).to_dict()\n",
    "    self.file_list = list(self.path.rglob('*.wav'))\n",
    "    \n",
    "  def __getitem__(self, idx):\n",
    "    fn = self.file_list[idx]\n",
    "    audio, sr = torchaudio.load(fn)\n",
    "    pitch = self.meta[fn.stem]['pitch']\n",
    "    pitch = torch.tensor(pitch, dtype=torch.long)\n",
    "    return audio, pitch\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.meta.keys())\n",
    "\n",
    "dataset = NSynthDataSet(Path('nsynth-test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4add8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('nsynth-test/examples.json')\n",
    "df['bass_synthetic_068-049-025']['pitch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5276d5d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.meta[dataset.file_list[0].stem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204a1bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio, pitch = dataset[2000]\n",
    "ipd.Audio(audio, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0c06c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de273c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset, batch_size=128, num_workers=4, shuffle=True, pin_memory=True)\n",
    "# test_loader = DataLoader(testset, batch_size=128, shuffle=False, num_workers=4)\n",
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9929be21",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio, pitch = batch\n",
    "\n",
    "audio.shape, pitch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3597f5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = torch.arange(49).view(1, 7,7).float()\n",
    "\n",
    "plt.imshow(dummy[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45feb6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = 3\n",
    "padding_size= 2\n",
    "stride_size = 2\n",
    "conv_layer = nn.Conv2d(1, 1, kernel_size, padding=padding_size, stride=stride_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c3865e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_output = conv_layer(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f319755",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(conv_output[0].detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0535cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel_size = 3\n",
    "padding_size= 2\n",
    "# stride_size = 2\n",
    "\n",
    "conv_t_layer = nn.ConvTranspose2d(1,1, kernel_size, padding=padding_size, stride=stride_size)\n",
    "t_output = conv_t_layer(conv_output)\n",
    "\n",
    "plt.imshow(t_output[0].detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7642930",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_t_layer(conv_layer(dummy)).shape, dummy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9650454b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpecModel(nn.Module):\n",
    "  def __init__(self, n_fft, hop_length):\n",
    "    super().__init__()\n",
    "    self.spec_converter = torchaudio.transforms.Spectrogram(n_fft=n_fft, hop_length=hop_length)\n",
    "    self.db_converter = torchaudio.transforms.AmplitudeToDB(stype='power')\n",
    "\n",
    "  def forward(self, audio_sample):\n",
    "    spec = self.spec_converter(audio_sample)\n",
    "    db_spec = self.db_converter(spec)\n",
    "    return db_spec\n",
    "\n",
    "class Conv2dNormPool(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, kernel_size, padding, stride):\n",
    "    super().__init__()\n",
    "    self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, padding=padding, stride=stride)\n",
    "    self.batch_norm = nn.BatchNorm2d(out_channels)\n",
    "    self.activation = nn.ReLU()\n",
    "    \n",
    "  def forward(self, x):\n",
    "    x = self.conv(x)\n",
    "    x = self.batch_norm(x)\n",
    "    x = self.activation(x)\n",
    "    return x\n",
    "  \n",
    "class Conv2dNormTransposePool(Conv2dNormPool):\n",
    "  def __init__(self, in_channels, out_channels, kernel_size, padding, stride):\n",
    "    super().__init__(in_channels, out_channels, kernel_size, padding, stride)\n",
    "    self.conv = nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, padding=padding, stride=stride)\n",
    "\n",
    "  \n",
    "class AutoEncoder(nn.Module):\n",
    "  def __init__(self, n_fft, hop_length, hidden_size=256):\n",
    "    super().__init__()\n",
    "    self.spec_model = SpecModel(n_fft, hop_length)\n",
    "    self.encoder = nn.Sequential()\n",
    "    self.num_channels = [1] + [hidden_size // 2**i for i in reversed(range(7))]\n",
    "    for i in range(6):\n",
    "      self.encoder.add_module(f\"conv_norm{i}\", Conv2dNormPool(self.num_channels[i], self.num_channels[i+1], (4,4), 1, (2,2) ))\n",
    "    self.encoder.add_module(f\"final_conv\",nn.Conv2d(in_channels=self.num_channels[-2], out_channels=self.num_channels[-1], kernel_size=(3,3), padding=1))\n",
    "    self.final_layer = nn.Linear(hidden_size * 32, hidden_size) \n",
    "  \n",
    "    self.decoder = nn.Sequential(      \n",
    "        Conv2dNormTransposePool(in_channels=self.num_channels[-1] + hidden_size, out_channels=self.num_channels[-2], kernel_size=(8,4), padding=0, stride=(2,2))\n",
    "    )\n",
    "    for i in range(5):\n",
    "      self.decoder.add_module(f\"conv_norm{i}\", Conv2dNormTransposePool(self.num_channels[-2-i], self.num_channels[-3-i], (4,4), 1, (2,2)))\n",
    "    self.decoder.add_module(\"final_module\",  nn.ConvTranspose2d(in_channels=self.num_channels[1], out_channels=1, kernel_size=(4,4), padding=1, stride=(2,2)),)\n",
    "    self.pitch_embedder = nn.Embedding(121, hidden_size)\n",
    "    \n",
    "  def forward(self, x, pitch):\n",
    "    spec = self.spec_model(x)\n",
    "    spec = spec[:,:,:-1] # to match 512\n",
    "    spec /= 80\n",
    "    spec = nn.functional.pad(spec, (2,3), value=torch.min(spec))\n",
    "    out = self.encoder(spec)\n",
    "\n",
    "    latent = self.final_layer(out.view(out.shape[0], -1))\n",
    "    latent = torch.cat([latent, self.pitch_embedder(pitch)], dim=-1)\n",
    "    latent = latent.view(latent.shape[0], -1, 1, 1)\n",
    "    recon_spec = self.decoder(latent)\n",
    "    return recon_spec, spec\n",
    "  \n",
    "model = AutoEncoder(1024, 256, 1024)\n",
    "recon_spec, spec = model(audio, pitch)\n",
    "recon_spec.shape, spec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18178f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.num_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da31224",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74113ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.final_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d76916",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = model.spec_model(audio)\n",
    "spec.shape\n",
    "spec = spec[:,:,:-1] # to match 512\n",
    "spec /= 80\n",
    "spec = nn.functional.pad(spec, (2,3), value=torch.min(spec))\n",
    "out = model.encoder(spec)\n",
    "\n",
    "out = out.reshape(out.shape[0], out.shape[1], -1)\n",
    "latent = model.final_layer(out.view(out.shape[0], -1))\n",
    "latent = torch.cat([latent, pitch], dim=-1)\n",
    "latent = latent.unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "recon_spec = model.decoder(latent)\n",
    "spec.shape, recon_spec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1115e22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229f5f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a75ad0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044ea9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(pred, target):\n",
    "  return ((pred-target)**2).mean()\n",
    "\n",
    "loss_fn(recon_spec, spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ba9076",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedSpecLoss:\n",
    "  def __init__(self, fft_size=1024, sr=16000, device='cuda'):\n",
    "    self.weight = torch.ones(fft_size//2).to(device)\n",
    "    self.weight[:fft_size//4] = torch.linspace(10,1,fft_size//4)\n",
    "\n",
    "  def __call__(self, pred, target):\n",
    "    mse = (pred-target)**2\n",
    "    mse *= self.weight[:, None]\n",
    "    return mse.mean()\n",
    "  \n",
    "loss_calculator = WeightedSpecLoss()\n",
    "loss_calculator(recon_spec.cuda(), spec.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6958b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_calculator.weight.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58225a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "num_epochs = 5\n",
    "device = 'cuda'\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "train_loader = DataLoader(dataset, batch_size=8, num_workers=4, shuffle=True, pin_memory=True)\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "  for batch in train_loader:\n",
    "    audio, pitch = batch\n",
    "    recon_spec, spec = model(audio.to(device), pitch.to(device))\n",
    "    loss = loss_calculator(recon_spec, spec)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6d4f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'autoencoder.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c5f3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(recon_spec[4,0].detach().cpu(), origin='lower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf23816",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown 14VsTi0tqKB7NFJQca9QpA-_envK212kn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb9adb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weights = torch.load('note_autoencoder_best.pt', map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819e26fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da857c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(dataset, batch_size=64, num_workers=4,pin_memory=True)\n",
    "\n",
    "test_batch = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c1f9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cpu()\n",
    "audio, pitch = test_batch\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  recon_spec, spec = model(audio, pitch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626b78e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_id = 0\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(recon_spec[sample_id, 0], origin='lower', aspect='auto')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(spec[sample_id, 0], origin='lower', aspect='auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616b7e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = model.spec_model(audio)\n",
    "spec.shape\n",
    "spec = spec[:,:,:-1] # to match 512\n",
    "spec /= 80\n",
    "spec = nn.functional.pad(spec, (2,3), value=torch.min(spec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f642e2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66084e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_output_to_audio(spec):\n",
    "  rescaled_spec = spec * 80\n",
    "  padded_spec = nn.functional.pad(rescaled_spec, (0,0, 0,1), value=-100)\n",
    "  magnitude_spec = torchaudio.functional.DB_to_amplitude(padded_spec, ref=1, power=1)\n",
    "  griffin_lim = torchaudio.transforms.GriffinLim(n_fft=1024, hop_length=256, n_iter=100)\n",
    "  spec_recon_audio = griffin_lim(magnitude_spec)\n",
    "  \n",
    "  return spec_recon_audio\n",
    "\n",
    "recon_audio = network_output_to_audio(recon_spec[10])\n",
    "ipd.Audio(recon_audio, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3551a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rescaled_spec = spec * 80\n",
    "padded_spec = nn.functional.pad(rescaled_spec, (0,0, 0,1), value=-100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966c840c",
   "metadata": {},
   "outputs": [],
   "source": [
    "magnitude_spec = torchaudio.functional.DB_to_amplitude(padded_spec, ref=1, power=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6875c94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(magnitude_spec[sample_id, 0], origin='lower', aspect='auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad34bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "magnitude_spec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa17bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "griffin_lim = torchaudio.transforms.GriffinLim(n_fft=1024, hop_length=256, n_iter=100)\n",
    "spec_recon_audio = griffin_lim(magnitude_spec[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e4cd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(spec_recon_audio, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c26d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(audio[sample_id], rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fceaca91",
   "metadata": {},
   "outputs": [],
   "source": [
    "??torchaudio.transforms.GriffinLim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bcfd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8ef020",
   "metadata": {},
   "outputs": [],
   "source": [
    "sound_a = audio[10:11] \n",
    "sound_b = audio[22:23]\n",
    "pitch_a = pitch[0:1]\n",
    "\n",
    "ipd.display(ipd.Audio(sound_a.squeeze(), rate=16000))\n",
    "ipd.display(ipd.Audio(sound_b.squeeze(), rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa05f436",
   "metadata": {},
   "outputs": [],
   "source": [
    "sound_c = (sound_a + sound_b)/2\n",
    "ipd.Audio(sound_c.squeeze(), rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1cfccf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2ec418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(model, x):\n",
    "  spec = model.spec_model(x)\n",
    "  spec = spec[:,:,:-1] # to match 512\n",
    "  spec /= 80\n",
    "  spec = nn.functional.pad(spec, (2,3), value=torch.min(spec))\n",
    "  out = model.encoder(spec)\n",
    "\n",
    "  latent = model.final_layer(out.view(out.shape[0], -1))\n",
    "  return latent\n",
    "\n",
    "embedding_a = get_embedding(model, sound_a)\n",
    "embedding_b = get_embedding(model, sound_b)\n",
    "embedding_c = get_embedding(model, sound_c)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c8db67",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_a.shape, embedding_b.shape, pitch_a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5115b4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mixed_embedding = (embedding_a + embedding_b)/2\n",
    "# mixed_embedding = (embedding_a * 0.7 + embedding_b *0.3)\n",
    "# mixed_embedding = embedding_c\n",
    "mixed_embedding = embedding_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b4742e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pitch_b = torch.zeros(1, 120)\n",
    "pitch_b[0, 83] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0880868f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding(model, latent, pitch):\n",
    "  latent = torch.cat([latent, pitch], dim=-1)\n",
    "  latent = latent.view(latent.shape[0], -1, 1, 1)\n",
    "  recon_spec = model.decoder(latent)\n",
    "  return recon_spec\n",
    "\n",
    "mixed_spec = decoding(model, mixed_embedding, pitch_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e93783",
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_audio = network_output_to_audio(mixed_spec)\n",
    "ipd.Audio(mixed_audio.detach().squeeze(), rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efd2675",
   "metadata": {},
   "outputs": [],
   "source": [
    "pitch_embedding_dim = 512\n",
    "# pitch_embedder = nn.Embedding(120, pitch_embedding_dim)\n",
    "pitch_embedder = nn.Linear(120, pitch_embedding_dim, bias=False)\n",
    "pitch_embedding = pitch_embedder(pitch_a)\n",
    "\n",
    "cat_embedding = torch.cat([mixed_embedding, pitch_embedding], dim=-1)\n",
    "# cat_embedding = torch.cat([mixed_embedding, pitch_a], dim=-1)\n",
    "\n",
    "cat_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cc7d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pitch_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438c8bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.decoder[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14cb2aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
